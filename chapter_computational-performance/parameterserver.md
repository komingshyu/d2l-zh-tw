# 引數伺服器
:label:`sec_parameterserver`

當我們從一個GPU遷移到多個GPU時，以及再遷移到包含多個GPU的多個伺服器時（可能所有伺服器的分佈跨越了多個機架和多個網路交換機），分散式並行訓練演算法也需要變得更加複雜。透過細節可以知道，一方面是不同的互連方式的頻寬存在極大的區別（例如，NVLink可以透過設定實現跨$6$條鏈路的高達100GB/s的頻寬，16通道的PCIe4.0提供32GB/s的頻寬，而即使是高速100GbE乙太網也只能提供大約10GB/s的頻寬）；另一方面是期望開發者既能完成統計學習建模還精通系統和網路也是不切實際的。

引數伺服器的核心思想首先是由 :cite:`Smola.Narayanamurthy.2010`在分散式隱變數模型的背景下引入的。然後，在 :cite:`Ahmed.Aly.Gonzalez.ea.2012`中描述了Push和Pull的語義，又在 :cite:`Li.Andersen.Park.ea.2014`中描述了系統和開源庫。下面，我們將介紹用於提高計算效率的元件。

## 資料並行訓練

讓我們回顧一下在分散式架構中資料並行的訓練方法，因為在實踐中它的實現相對簡單，因此本節將排除其他內容只對其進行介紹。由於當今的GPU擁有大量的視訊記憶體，因此在實際場景中（不包括圖深度學習）只有資料並行這種並行訓練策略值得推薦。圖 :numref:`fig_parameterserver`描述了在 :numref:`sec_multi_gpu`中實現的資料並行的變體。其中的關鍵是梯度的聚合需要在單個GPU（GPU 0）上完成，然後再將更新後的引數廣播給所有GPU。

![左圖是單GPU訓練；右圖是多GPU訓練的一個變體：（1）計算損失和梯度，（2）所有梯度聚合在一個GPU上，（3）發生引數更新，並將引數重新廣播給所有GPU](../img/ps.svg)
:label:`fig_parameterserver`

回顧來看，選擇GPU 0進行聚合似乎是個很隨便的決定，當然也可以選擇CPU上聚合，事實上只要最佳化演算法支援，在實際操作中甚至可以在某個GPU上聚合其中一些引數，而在另一個GPU上聚合另一些引數。例如，如果有四個與引數向量相關的梯度$\mathbf{g}_1, \ldots, \mathbf{g}_4$，還可以一個GPU對一個$\mathbf{g}_i (i = 1, \ldots, 4$）地進行梯度聚合。

這樣的推斷似乎是輕率和武斷的，畢竟數學應該是邏輯自洽的。但是，我們處理的是如 :numref:`sec_hardware`中所述的真實的物理硬體，其中不同的匯流排具有不同的頻寬。考慮一個如 :numref:`sec_hardware`中所述的真實的$4$路GPU伺服器。如果它的連線是特別完整的，那麼可能擁有一個100GbE的網絡卡。更有代表性的數字是1-10GbE範圍內，其有效頻寬為100MB/s到1GB/s。因為CPU的PCIe通道太少（例如，消費級的Intel CPU有$24$個通道），所以無法直接與所有的GPU相連線，因此需要[multiplexer](https://www.broadcom.com/products/pcie-switches-bridges/pcie-switches)。CPU在16x Gen3鏈路上的頻寬為16GB/s，這也是每個GPU連線到交換機的速度，這意味著GPU裝置之間的通訊更有效。

![一個4路GPU伺服器](../img/bw-hierarchy.svg)
:label:`fig_bw_hierarchy`

為了便於討論，我們假設所有梯度共需160MB。在這種情況下，將其中$3$個GPU的梯度傳送到第$4$個GPU上需要$30$毫秒（每次傳輸需要$10$毫秒=160MB/16GB/s）。再加上$30$毫秒將權重向量傳輸回來，得到的結果是總共需要$60$毫秒。如果將所有的資料傳送到CPU，總共需要$80$毫秒，其中將有$40$毫秒的懲罰，因為$4$個GPU每個都需要將資料傳送到CPU。最後，假設能夠將梯度分為$4$個部分，每個部分為$40$MB，現在可以在不同的GPU上同時聚合每個部分。因為PCIe交換機在所有鏈路之間提供全頻寬操作，所以傳輸需要$2.5\times 3=7.5$毫秒，而不是$30$毫秒，因此同步操作總共需要$15$毫秒。簡而言之，一樣的引數同步操作基於不同的策略時間可能在$15$毫秒到$80$毫秒之間。 :numref:`fig_ps_distributed`描述了交換引數的不同策略。

![引數同步策略](../img/ps-distributed.svg)
:label:`fig_ps_distributed`

請注意，我們還可以使用另一個工具來改善效能：在深度網路中，從頂部到底部計算所有梯度需要一些時間，因此即使還在忙著為某些引數計算梯度時，就可以開始為準備好的引數同步梯度了。想了解詳細資訊可以參見 :cite:`Sergeev.Del-Balso.2018`，想知道如何操作可參考[Horovod](https://github.com/horovod/horovod)。

## 環同步（Ring Synchronization）

當談及現代深度學習硬體的同步問題時，我們經常會遇到大量的客製的網路連線。例如，AWS p3.16xlarge和NVIDIA DGX-2例項中的連線都使用了 :numref:`fig_nvlink`中的結構。每個GPU透過PCIe鏈路連線到主機CPU，該鏈路最多隻能以16GB/s的速度執行。此外，每個GPU還具有$6$個NVLink連線，每個NVLink連線都能夠以300Gbit/s進行雙向傳輸。這相當於每個鏈路每個方向約$300\div 8\div 2\approx 18 \mathrm{GB/s}$。簡言之，聚合的NVLink頻寬明顯高於PCIe頻寬，問題是如何有效地使用它。

![在8台V100 GPU伺服器上連線NVLink（圖片由英偉達提供）](../img/nvlink.svg)
:label:`fig_nvlink`

 :cite:`Wang.Li.Liberty.ea.2018`的研究結果表明最優的同步策略是將網路分解成兩個環，並基於兩個環直接同步資料。
 :numref:`fig_nvlink_twoloop`描述了網路可以分解為一個具有雙NVLink頻寬的環（1-2-3-4-5-6-7-8-1）和一個具有常規頻寬的環（1-4-6-3-5-8-2-7-1）。在這種情況下，設計一個高效的同步協議是非常重要的。

![將NVLink網路分解為兩個環。](../img/nvlink-twoloop.svg)
:label:`fig_nvlink_twoloop`

考慮下面的思維試驗：給定由$n$個計算節點（或GPU）組成的一個環，梯度可以從第一個節點發送到第二個節點，在第二個結點將本地的梯度與傳送的梯度相加併發送到第三個節點，依此類推。在$n-1$步之後，可以在最後存取的節點中找到聚合梯度。也就是說，聚合梯度的時間隨節點數線性增長。但如果照此操作，演算法是相當低效的。歸根結底，在任何時候都只有一個節點在通訊。如果我們將梯度分為$n$個塊，並從節點$i$開始同步塊$i$，會怎麼樣？因為每個塊的大小是$1/n$，所以總時間現在是$(n-1)/n \approx 1$。換句話說，當我們增大環的大小時，聚合梯度所花費的時間不會增加。這是一個相當驚人的結果。 :numref:`fig_ringsync`說明了$n=4$個節點上的步驟順序。

![跨4個節點的環同步。每個節點開始向其左鄰居傳送部分梯度，直到在其右鄰居中找到聚合的梯度](../img/ringsync.svg)
:label:`fig_ringsync`

如果我們使用相同的例子，跨$8$個V100 GPU同步160MB，我們得到的結果大約是$2 \times 160 \mathrm{MB} \div (3 \times18 \mathrm{GB/s}) \approx 6 \mathrm{ms}$。這比使用PCIe匯流排要好，即使我們現在使用的是$8$個GPU。請注意，這些數字在實踐中通常會差一些，因為深度學習框架無法將通訊組合成大的突發傳輸。

注意到有一種常見的誤解認為環同步與其他同步演算法在本質上是不同的，實際上與簡單的樹演算法相比其唯一的區別是同步路徑稍微精細一些。

## 多機訓練

新的挑戰出現在多臺機器上進行分散式訓練：我們需要伺服器之間相互通訊，而這些伺服器又只通過相對較低的頻寬結構連線，在某些情況下這種連線的速度可能會慢一個數量級，因此跨裝置同步是個棘手的問題。畢竟，在不同機器上執行訓練程式碼的速度會有細微的差別，因此如果想使用分散式最佳化的同步演算法就需要*同步*（synchronize）這些機器。
 :numref:`fig_ps_multimachine`說明了分散式並行訓練是如何發生的。

1. 在每臺機器上讀取一組（不同的）批次資料，在多個GPU之間分割資料並傳輸到GPU的視訊記憶體中。基於每個GPU上的批次資料分別計算預測和梯度。
2. 來自一臺機器上的所有的本地GPU的梯度聚合在一個GPU上（或者在不同的GPU上聚合梯度的某些部分）。
3. 每臺機器的梯度被髮送到其本地CPU中。
4. 所有的CPU將梯度傳送到中央引數伺服器中，由該伺服器聚合所有梯度。
5. 然後使用聚合後的梯度來更新引數，並將更新後的引數廣播回各個CPU中。
6. 更新後的引數資訊傳送到本地一個（或多個）GPU中。
7. 所有GPU上的引數更新完成。

![多機多GPU分散式並行訓練](../img/ps-multimachine.svg)
:label:`fig_ps_multimachine`

以上這些操作似乎都相當簡單，而且事實上它們可以在一臺機器內高效地執行，但是當我們考慮多臺機器時，就會發現中央的引數伺服器成為了瓶頸。畢竟，每個伺服器的頻寬是有限的，因此對$m$個工作節點來說，將所有梯度傳送到伺服器所需的時間是$\mathcal{O}(m)$。我們也可以透過將引數伺服器數量增加到$n$來突破這一障礙。此時，每個伺服器只需要儲存$\mathcal{O}(1/n)$個引數，因此更新和最佳化的總時間變為$\mathcal{O}(m/n)$。這兩個數字的匹配會產生穩定的延展性，而不用在乎我們需要處理多少工作節點。在實際應用中，我們使用同一臺機器既作為工作節點還作為伺服器。設計說明請參考 :numref:`fig_ps_multips`（技術細節請參考 :cite:`Li.Andersen.Park.ea.2014`）。特別是，確保多臺機器只在沒有不合理延遲的情況下工作是相當困難的。

![上圖：單引數伺服器是一個瓶頸，因為它的頻寬是有限的；下圖：多引數伺服器使用聚合頻寬儲存部分引數](../img/ps-multips.svg)
:label:`fig_ps_multips`

## 鍵值儲存

在實踐中，實現分散式多GPU訓練所需要的步驟絕非易事。這就是公共抽象值得使用的原因，公共抽象即重新定義具有更新語義的*鍵－值儲存*（key-value store）的抽象。

在許多工作節點和許多GPU中，梯度$i$的計算可以定義為

$$\mathbf{g}_{i} = \sum_{k \in \text{workers}} \sum_{j \in \text{GPUs}} \mathbf{g}_{ijk},$$

其中$\mathbf{g}_{ijk}$是在工作節點$k$的GPU$j$上拆分的梯度$i$的一部分。這個運算的關鍵在於它是一個*交換歸約*（commutative reduction），也就是說，它把許多向量變換成一個向量，而運算順序在完成向量變換時並不重要。這對實現我們的目標來說是非常好的，因為不需要為何時接收哪個梯度進行細粒度的控制。此外，請注意，這個操作在不同的$i$之間是獨立的。

這就允許我們定義下面兩個操作：*push*（用於累積梯度）和*pull*（用於取得聚合梯度）。因為我們有很多層，也就有很多不同的梯度集合，因此需要用一個鍵$i$來對梯度建索引。這個與Dynamo :cite:`DeCandia.Hastorun.Jampani.ea.2007`中引入的*鍵－值儲存*之間存在相似性並非巧合。它們兩個定義都擁有許多相似的性質，特別是在多個伺服器之間分發引數時。

*鍵－值儲存*的push與pull操作描述如下：

* **push（key，value）**將特定的梯度值從工作節點發送到公共儲存，在那裡透過某種方式（例如，相加）來聚合值；
* **pull（key，value）**從公共儲存中取得某種方式（例如，組合來自所有工作節點的梯度）的聚合值。

透過將同步的所有複雜性隱藏在一個簡單的push和pull操作背後，我們可以將統計建模人員（他們希望能夠用簡單的術語表達最佳化）和系統工程師（他們需要處理分散式同步中固有的複雜性）的關注點解耦。

## 小結

* 同步需要高度適應特定的網路基礎設施和伺服器內的連線，這種適應會嚴重影響同步所需的時間。
* 環同步對於p3和DGX-2伺服器是最佳的，而對於其他伺服器則未必。
* 當新增多個引數伺服器以增加頻寬時，分層同步策略可以工作的很好。

## 練習

1. 請嘗試進一步提高環同步的效能嗎。（提示：可以雙向傳送訊息。）
1. 在計算仍在進行中，可否允許執行非同步通訊？它將如何影響效能？
1. 怎樣處理在長時間執行的計算過程中丟失了一臺伺服器這種問題？嘗試設計一種容錯機制來避免重啟計算這種解決方案？

[Discussions](https://discuss.d2l.ai/t/5774)
