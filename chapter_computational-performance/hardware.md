# 硬體
:label:`sec_hardware`

很好地理解演算法和模型才可以捕獲統計方面的問題，構建出具有出色效能的系統。同時，至少對底層硬體有一定的瞭解也是必不可少的。本節不能替代硬體和系統設計的相關課程。相反，本節的內容可以作為理解某些演算法為什麼比其他演算法更高效以及如何實現良好吞吐量的起點。一個好的設計可以很容易地在效能上造就數量級的差異，這也是後續產生的能夠訓練網路（例如，訓練時間為$1$周）和無法訓練網路（訓練時間為$3$個月，導致錯過截止期）之間的差異。我們先從計算機的研究開始。然後深入檢視CPU和GPU。最後，再檢視資料中心或雲中的多臺計算機的連線方式。

![每個程式設計師都應該知道的延遲數字](../img/latencynumbers.png)
:label:`fig_latencynumbers`

也可以透過 :numref:`fig_latencynumbers`進行簡單的瞭解，圖片源自科林·斯科特的[互動帖子](https://people.eecs.berkeley.edu/~rcs/research/interactive_latency.html)，在帖子中很好地概述了過去十年的進展。原始的數字是取自於傑夫迪恩的[Stanford講座](https://static.googleusercontent.com/media/research.google.com/en//people/jeff/Stanford-DL-Nov-2010.pdf)。下面的討論解釋了這些數字的一些基本原理，以及它們如何指導我們去設計算法。下面的討論是非常籠統和粗略的。很顯然，它並不能代替一門完整的課程，而只是為了給統計建模者提供足夠的資訊，讓他們做出合適的設計決策。對於計算機體系結構的深入概述，建議讀者參考 :cite:`Hennessy.Patterson.2011`或關於該主題的最新課程，例如[Arste Asanovic](http://inst.eecs.berkeley.edu/~cs152/sp19/)。

## 計算機

大多數深度學習研究者和實踐者都可以使用一臺具有相當數量的記憶體、計算資源、某種形式的加速器（如一個或者多個GPU）的計算機。計算機由以下關鍵部件組成：

* 一個處理器（也被稱為CPU），它除了能夠執行作業系統和許多其他功能之外，還能夠執行給定的程式。它通常由$8$個或更多個核心組成；
* 記憶體（隨機存取儲存，RAM）用於儲存和檢索計算結果，如權重向量和啟用引數，以及訓練資料；
* 一個或多個乙太網連線，速度從1GB/s到100GB/s不等。在高階伺服器上可能用到更進階的互連；
* 高速擴充匯流排（PCIe）用於系統連線一個或多個GPU。伺服器最多有$8$個加速卡，通常以更進階的拓撲方式連線，而桌面系統則有$1$個或$2$個加速卡，具體取決於使用者的預算和電源負載的大小；
* 永續性儲存裝置，如磁碟驅動器、固態驅動器，在許多情況下使用高速擴充匯流排連線。它為系統需要的訓練資料和中間檢查點需要的儲存提供了足夠的傳輸速度。

![計算機元件的連線](../img/mobo-symbol.svg)
:label:`fig_mobo-symbol`

如 :numref:`fig_mobo-symbol`所示，高速擴充匯流排由直接連線到CPU的多個通道組成，將CPU與大多陣列件（網路、GPU和儲存）連線在一起。例如，AMD的Threadripper3有$64$個PCIe4.0通道，每個通道都能夠雙向傳輸16Gbit/s的資料。記憶體直接連線到CPU，總頻寬高達100GB/s。

當我們在計算機上執行程式碼時，需要將資料轉移到處理器上（CPU或GPU）執行計算，然後將結果從處理器移回到隨機存取儲存和持久儲存器中。因此，為了獲得良好的效能，需要確保每一步工作都能無縫連結，而不希望系統中的任何一部分成為主要的瓶頸。例如，如果不能快速載入圖像，那麼處理器就無事可做。同樣地，如果不能快速移動矩陣到CPU（或GPU）上，那麼CPU（或GPU）就會無法全速執行。最後，如果希望在網路上同步多臺計算機，那麼網路就不應該拖累計算速度。一種選擇是通訊和計算交錯進行。接下來將詳細地介紹各個元件。

## 記憶體

最基本的記憶體主要用於儲存需要隨時存取的資料。目前，CPU的記憶體通常為[DDR4](https://en.wikipedia.org/wiki/DDR4_SDRAM)型別，每個模組提供20-25Gb/s的頻寬。每個模組都有一條$64$位寬的匯流排。通常使用成對的記憶體模組來允許多個通道。CPU有$2$到$4$個記憶體通道，也就是說，它們記憶體頻寬的峰值在40GB/s到100GB/s之間。一般每個通道有兩個物理儲存體（bank）。例如AMD的Zen 3 Threadripper有$8$個插槽。

雖然這些數字令人印象深刻，但實際上它們只能說明了一部分故事。當我們想要從記憶體中讀取一部分內容時，需要先告訴記憶體模組在哪裡可以找到資訊。也就是說，我們需要先將*地址*（address）傳送到RAM。然後我們可以選擇唯讀取一條$64$位記錄還是一長串記錄。後者稱為*突發讀取*（burst read）。概括地說，向記憶體傳送地址並設定傳輸大約需要100ns（細節取決於所用記憶體晶片的特定定時係數），每個後續傳輸只需要0.2ns。總之，第一次讀取的成本是後續讀取的500倍！請注意，每秒最多可以執行一千萬次隨機讀取。這說明應該儘可能地避免隨機記憶體存取，而是使用突發模式讀取和寫入。

當考慮到擁有多個物理儲存體時，事情就更加複雜了。每個儲存體大部分時候都可以獨立地讀取記憶體。這意味著兩件事。一方面，如果隨機讀操作均勻分佈在記憶體中，那麼有效的隨機讀操作次數將高達4倍。這也意味著執行隨機讀取仍然不是一個好主意，因為突發讀取的速度也快了4倍。另一方面，由於記憶體對齊是$64$位邊界，因此最好將任何資料結構與相同的邊界對齊。當設定了適當的標誌時，編譯器基本上就是[自動化](https://en.wikipedia.org/wiki/Data_structure_alignment)地執行對齊操作。我們鼓勵好奇的讀者回顧一下[Zeshan Chishti關於DRAM的講座](http://web.cecs.pdx.edu/~zeshan/ece585_lec5.pdf)。

GPU記憶體的頻寬要求甚至更高，因為它們的處理單元比CPU多得多。總的來說，解決這些問題有兩種選擇。首先是使記憶體匯流排變得更寬。例如，NVIDIA的RTX 2080Ti有一條352位寬的匯流排。這樣就可以同時傳輸更多的資訊。其次，GPU使用特定的高效能記憶體。消費級裝置，如NVIDIA的RTX和Titan系列，通常使用[GDDR6](https://en.wikipedia.org/wiki/GDDR6_SDRAM）晶片，總頻寬超過500GB/s。另一種選擇是使用HBM（高頻寬儲存器)模組。它們使用截然不同的介面，直接與專用矽片上的GPU連線。這使得它們非常昂貴，通常僅限於高階伺服器晶片，如NVIDIA Volta V100系列加速卡。毫不意外的是GPU的記憶體通常比CPU的記憶體小得多，因為前者的成本更高。就目的而言，它們的效能與特徵大體上是相似的，只是GPU的速度更快。就本書而言，我們完全可以忽略細節，因為這些技術只在調整GPU核心以獲得高吞吐量時才起作用。

## 儲存器

隨機存取儲存的一些關鍵特性是 *頻寬*（bandwidth）和 *延遲*（latency）。儲存裝置也是如此，只是不同裝置之間的特性差異可能更大。

### 硬碟驅動器

*硬碟驅動器*（hard disk drive，HDD）已經使用了半個多世紀。簡單的說，它們包含許多旋轉的碟片，這些碟片的磁頭可以放置在任何給定的磁軌上進行讀寫。高階磁碟在$9$個碟片上可容納高達16TB的容量。硬碟的主要優點之一是相對便宜，而它們的眾多缺點之一是典型的災難性故障模式和相對較高的讀取延遲。

要理解後者，請了解一個事實即硬碟驅動器的轉速大約為7200RPM（每分鐘轉數）。它們如果轉速再快些，就會由於施加在碟片上的離心力而破碎。在存取磁碟上的特定扇區時，還有一個關鍵問題：需要等待碟片旋轉到位（可以移動磁頭，但是無法對磁碟加速）。因此，可能需要$8$毫秒才能使用請求的資料。一種常見的描述方式是，硬碟驅動器可以以大約100IOPs（每秒輸入/輸出操作）的速度工作，並且在過去二十年中這個數字基本上沒變。同樣糟糕的是，頻寬（大約為100-200MB/s）也很難增加。畢竟，每個磁頭讀取一個磁軌的位元，因此位元率只隨資訊密度的平方根縮放。因此，對於非常大的資料集，HDD正迅速降級為歸檔儲存和低階儲存。

### 固態驅動器

固態驅動器（solid state drives，SSD）使用快閃記憶體持久地儲存資訊。這允許更快地存取儲存的記錄。現代的固態驅動器的IOPs可以達到$10$萬到$50$萬，比硬碟驅動器快3個數量級。而且，它們的頻寬可以達到1-3GB/s，比硬碟驅動器快一個數量級。這些改進聽起來好的難以置信，而事實上受固態驅動器的設計方式，它仍然存在下面的附加條件。

* 固態驅動器以塊的方式（256KB或更大）儲存資訊。塊只能作為一個整體來寫入，因此需要耗費大量的時間，導致固態驅動器在按位隨機寫入時效能非常差。而且通常資料寫入需要大量的時間還因為塊必須被讀取、擦除，然後再重新寫入新的資訊。如今固態驅動器的控制器和韌體已經開發出了緩解這種情況的演算法。儘管有了演算法，寫入速度仍然會比讀取慢得多，特別是對於QLC（四層單元）固態驅動器。提高效能的關鍵是維護操作的“佇列”，在佇列中儘可能地優先讀取和寫入大的塊。
* 固態驅動器中的儲存單元磨損得比較快（通常在幾千次寫入之後就已經老化了）。磨損程度保護演算法能夠將退化平攤到許多單元。也就是說，不建議將固態驅動器用於交換分割槽檔案或大型日誌檔案。
* 最後，頻寬的大幅增加迫使計算機設計者將固態驅動器與PCIe匯流排相連線，這種驅動器稱為NVMe（非易失性記憶體增強），其最多可以使用$4$個PCIe通道。在PCIe4.0上最高可達8GB/s。

### 雲端儲存

雲端儲存提供了一系列可配置的效能。也就是說，虛擬機器的儲存在數量和速度上都能根據使用者需要進行動態分配。建議使用者在延遲太高時（例如，在訓練期間存在許多小記錄時）增加IOPs的配置數。

## CPU

中央處理器（central processing unit，CPU）是任何計算機的核心。它們由許多關鍵元件組成：*處理器核心*（processor cores）用於執行機器程式碼的；*匯流排*（bus）用於連線不同元件（注意，匯流排會因為處理器型號、各代產品和供應商之間的特定拓撲結構有明顯不同）；*快取*（cach）相比主記憶體實現更高的讀取頻寬和更低的延遲記憶體存取。最後，因為高效能線性代數和卷積運算常見於媒體處理和機器學習中，所以幾乎所有的現代CPU都包含*向量處理單元*（vector processing unit）為這些計算提供輔助。

![Intel Skylake消費級四核CPU](../img/skylake.svg)
:label:`fig_skylake`

 :numref:`fig_skylake`描述了Intel Skylake消費級四核CPU。它包含一個整合GPU、快取和一個連線四個核心的環匯流排。例如，乙太網、WiFi、藍芽、SSD控制器和USB這些外圍裝置要麼是晶片組的一部分，要麼透過PCIe直接連線到CPU。

### 微體系結構

每個處理器核心都由一組相當複雜的元件組成。雖然不同時代的產品和供應商的細節有所不同，但基本功能都是標準的。前端載入指令並嘗試預測將採用哪條路徑（例如，為了控制流），然後將指令從彙編程式碼解碼為微指令。彙編程式碼通常不是處理器執行的最低級別程式碼，而複雜的微指令卻可以被解碼成一組更低階的操作，然後由實際的執行核心處理。通常執行核心能夠同時執行許多操作，例如， :numref:`fig_cortexa77`的ARM Cortex A77核心可以同時執行多達$8$個操作。

![ARM Cortex A77微體系結構](../img/a77.svg)
:label:`fig_cortexa77`

這意味著高效的程式可以在每個時鐘週期內執行多條指令，前提是這些指令可以獨立執行。不是所有的處理單元都是平等的。一些專用於處理整數指令，而另一些則針對浮點效能進行了最佳化。為了提高吞吐量，處理器還可以在分支指令中同時執行多條程式碼路徑，然後丟棄未選擇分支的結果。這就是為什麼前端的分支預測單元很重要，因為只有最有希望的路徑才會被繼續執行。

### 向量化

深度學習的計算量非常大。因此，為了滿足機器學習的需要，CPU需要在一個時鐘週期內執行許多操作。這種執行方式是透過向量處理單元實現的。這些處理單元有不同的名稱:在ARM上叫做NEON,在x86上被稱為[AVX2](https://en.wikipedia.org/wiki/Advanced_Vector_Extensions)。一個常見的功能是它們能夠執行單指令多資料（single instruction multiple data，SIMD）操作。 :numref:`fig_neon128`顯示瞭如何在ARM上的一個時鐘週期中完成$8$個整數加法。

![128位NEON向量化](../img/neon128.svg)
:label:`fig_neon128`

根據體系結構的選擇，此類暫存器最長可達$512$位，最多可組合$64$對數字。例如，我們可能會將兩個數字相乘，然後與第三個數字相加，這也稱為乘加融合（fused multiply-add）。Intel的[OpenVino](https://01.org/openvinotoolkit)就是使用這些處理器來獲得可觀的吞吐量，以便在伺服器級CPU上進行深度學習。不過請注意，這個數字與GPU的能力相比則相形見絀。例如，NVIDIA的RTX 2080Ti擁有$4352$個CUDA核心，每個核心都能夠在任何時候處理這樣的操作。

### 快取

考慮以下情況：我們有一箇中等規模的$4$核心的CPU，如 :numref:`fig_skylake`所示，執行在2GHz頻率。此外，假設向量處理單元啟用了$256$位頻寬的AVX2，其IPC（指令/時鐘）計數為1。進一步假設從記憶體中獲取用於AVX2操作的指令至少需要一個暫存器。這意味著CPU每個時鐘週期需要消耗$4 \times 256 \text{ bit} = 128 \text{ bytes}$的資料。除非我們能夠每秒向處理器傳輸$2 \times 10^9 \times 128 = 256 \times 10^9$位元組，否則用於處理的資料將會不足。不幸的是，這種晶片的儲存器介面僅支援20-40Gb/s的資料傳輸，即少了一個數量級。解決方法是儘可能避免從記憶體中載入新資料，而是將資料放在CPU的快取上。這就是使用快取的地方。通常使用以下名稱或概念。

* **暫存器**，嚴格來說不是快取的一部分，用於幫助組織指令。也就是說，暫存器是CPU可以以時鐘速度存取而沒有延遲的儲存位置。CPU有幾十個暫存器，因此有效地使用暫存器取決於編譯器（或程式設計師）。例如，C語言有一個`register`關鍵字。
* **一級快取**是應對高記憶體頻寬要求的第一道防線。一級快取很小（常見的大小可能是32-64KB），內容通常分為資料和指令。當資料在一級快取中被找到時，其存取速度非常快，如果沒有在那裡找到，搜尋將沿著快取層次結構向下尋找。
* **二級快取**是下一站。根據架構設計和處理器大小的不同，它們可能是獨佔的也可能是共享的。即它們可能只能由給定的核心存取，或者在多個核心之間共享。二級快取比一級快取大（通常每個核心256-512KB），而速度也更慢。此外，我們首先需要檢查以確定資料不在一級快取中，才會存取二級快取中的內容，這會增加少量的額外延遲。
* **三級快取**在多個核之間共享，並且可以非常大。AMD的EPYC 3伺服器的CPU在多個晶片上擁有高達256MB的快取記憶體。更常見的數字在4-8MB範圍內。

預測下一步需要哪個儲存裝置是最佳化晶片設計的關鍵引數之一。例如，建議以*向前*的方向遍歷記憶體，因為大多數快取演算法將試圖*向前讀取*（read forward）而不是向後讀取。同樣，將記憶體存取模式保持在本地也是提高效能的一個好方法。

新增快取是一把雙刃劍。一方面，它能確保處理器核心不缺乏資料。但同時，它也增加了晶片尺寸，消耗了原本可以用來提高處理能力的面積。此外，*快取未命中*的代價可能會很昂貴。考慮最壞的情況，如 :numref:`fig_falsesharing`所示的*錯誤共享*（false sharing）。當處理器$1$上的執行緒請求資料時，記憶體位置快取在處理器$0$上。為了滿足獲取需要，處理器$0$需要停止它正在做的事情，將資訊寫回主記憶體，然後讓處理器$1$從記憶體中讀取它。在此操作期間，兩個處理器都需要等待。與高效的單處理器實現相比，這種程式碼在多個處理器上執行的速度可能要慢得多。這就是為什麼快取大小（除了物理大小之外）有實際限制的另一個原因。

![錯誤共享（圖片由英特爾提供）](../img/falsesharing.svg)
:label:`fig_falsesharing`

## GPU和其他加速卡

毫不誇張地說，如果沒有GPU，深度學習就不會成功。基於同樣的原因，有理由認為GPU製造商的財富由於深度學習而顯著增加。這種硬體和演算法的協同進化導致了這樣一種情況：無論好壞，深度學習都是更可取的統計建模範式。因此，瞭解GPU和其他加速卡（如TPU :cite:`Jouppi.Young.Patil.ea.2017`）的具體好處是值得的。

值得注意的是，在實踐中經常會有這樣一個判別：加速卡是為訓練還是推斷而最佳化的。對於後者，我們只需要計算網路中的前向傳播。而反向傳播不需要儲存中間資料。還有，我們可能不需要非常精確的計算（FP16或INT8通常就足夠了）。對於前者，即訓練過程中需要儲存所有的中間結果用來計算梯度。而且，累積梯度也需要更高的精度，以避免數值下溢（或溢位）。這意味著最低要求也是FP16（或FP16與FP32的混合精度）。所有這些都需要更快、更大的記憶體（HBM2或者GDDR6）和更高的處理能力。例如，NVIDIA優化了[Turing](https://devblogs.nvidia.com/nvidia-turing-architecture-in-depth/) T4 GPU用於推斷和V100 GPU用於訓練。

回想一下如 :numref:`fig_neon128`所示的向量化。處理器核心中新增向量處理單元可以顯著提高吞吐量。例如，在 :numref:`fig_neon128`的例子中，我們能夠同時執行$16$個操作。首先，如果我們新增的運算不僅優化了向量運算，而且優化了矩陣運算，會有什麼好處？稍後我們將討論基於這個策略引入的張量核（tensor cores）。第二，如果我們增加更多的核心呢？簡而言之，以上就是GPU設計決策中的兩種策略。 :numref:`fig_turing_processing_block`給出了基本處理塊的概述。它包含$16$個整數單位和$16$個浮點單位。除此之外，兩個張量核加速了與深度學習相關的附加操作的狹窄的子集。每個流式多處理器都由這樣的四個塊組成。

![NVIDIA Turing處理塊（圖片由英偉達提供）](../img/turing-processing-block.png)
:width:`150px`
:label:`fig_turing_processing_block`

接下來，將$12$個流式多處理器分組為圖形處理叢集，這些叢集構成了高階TU102處理器。充足的記憶體通道和二級快取完善了配置。 :numref:`fig_turing`有相關的細節。設計這種裝置的原因之一是可以根據需要獨立地新增或刪除模組，從而滿足設計更緊湊的晶片和處理良品率問題（故障模組可能無法啟用）的需要。幸運的是，在CUDA和框架程式碼層之下，這類裝置的程式設計對深度學習的臨時研究員隱藏得很好。特別是，只要有可用的資源GPU上就可以同時執行多個程式。儘管如此，瞭解裝置的侷限性是值得的，以避免對應的裝置記憶體的型號不合適。

![NVIDIA Turing架構（圖片由英偉達提供）](../img/turing.png)
:width:`350px`
:label:`fig_turing`

最後值得一提的是*張量核*（tensor core）。它們是最近增加更多最佳化電路趨勢的一個例子，這些最佳化電路對深度學習特別有效。例如，TPU添加了用於快速矩陣乘法的脈動陣列 :cite:`Kung.1988`，這種設計是為了支援非常小數量（第一代TPU支援數量為1）的大型操作。而張量核是另一個極端。它們針對$4 \times 4$和$16 \times 16$矩陣之間的小型運算進行了最佳化，具體取決於它們的數值精度。 :numref:`fig_tensorcore`給出了最佳化的概述。

![NVIDIA Turing架構中的張量核心（圖片由英偉達提供）](../img/tensorcore.jpg)
:width:`400px`
:label:`fig_tensorcore`

顯然，我們最終會在最佳化計算時做出某些妥協。其中之一是GPU不太擅長處理稀疏資料和中斷。儘管有一些明顯的例外，如[Gunrock](https://github.com/gunrock/gunrock) :cite:`Wang.Davidson.Pan.ea.2016`，但GPU擅長的高頻寬突發讀取操作並不適合稀疏的矩陣和向量的存取模式。存取稀疏資料和處理中斷這兩個目標是一個積極研究的領域。例如：[DGL](http://dgl.ai)，一個專為圖深度學習而設計的函式庫。

## 網路和匯流排

每當單個裝置不足以進行最佳化時，我們就需要來回傳輸資料以實現同步處理，於是網路和匯流排就派上了用場。我們有許多設計引數：頻寬、成本、距離和靈活性。應用的末端有WiFi，它有非常好的使用範圍，非常容易使用（畢竟沒有線纜），而且還便宜，但它提供的頻寬和延遲相對一般。頭腦正常的機器學習研究人員都不會用它來建構伺服器叢集。接下來的內容中將重點關注適合深度學習的互連方式。

* **PCIe**，一種專用匯流排，用於每個通道點到點連線的高頻寬需求（在$16$通道插槽中的PCIe4.0上高達32GB/s），延遲時間為個位數的微秒（5μs）。PCIe連結非常寶貴。處理器擁有的數量：AMD的EPYC 3有$128$個通道，Intel的Xeon每個晶片有$48$個通道；在桌面級CPU上，數字分別是$20$（Ryzen9）和$16$（Core i9）。由於GPU通常有$16$個通道，這就限制了以全頻寬與CPU連線的GPU數量。畢竟，它們還需要與其他高頻寬外圍裝置（如儲存和乙太網）共享鏈路。與RAM存取一樣，由於減少了資料套件的開銷，因此更適合大批次資料傳輸。
* **乙太網**，連線計算機最常用的方式。雖然它比PCIe慢得多，但它的安裝成本非常低，而且具有很強的彈性，覆蓋的距離也要長得多。低階伺服器的典型頻寬為1GBit/s。高階裝置（如雲中的[C5例項](https://aws.amazon.com/ec2/instance-types/c5/））提供10～100GBit/s的頻寬。與以前所有的情況一樣，資料傳輸有很大的開銷。請注意，原始乙太網幾乎從不被直接使用，而是在物理互連之上使用執行的協議（例如UDP或TCP/IP)。這進一步增加了開銷。與PCIe類似，乙太網旨在連線兩個裝置，例如計算機和交換機。
* **交換機**，一種連線多個裝置的方式，該連線方式下的任何一對裝置都可以同時執行（通常是全頻寬）點對點連線。例如，乙太網交換機可能以高頻寬連線$40$台伺服器。請注意，交換機並不是傳統計算機網路所獨有的。甚至PCIe通道也可以是[可交換的](https://www.broadcom.com/products/pcie-switches-bridges/pcie-switches)，例如：[P2例項](https://aws.amazon.com/ec2/instance-types/p2/)就是將大量GPU連線到主機處理器。
* **NVLink**，是PCIe的替代品，適用於非常高頻寬的互連。它為每條鏈路提供高達300Gbit/s的資料傳輸速率。伺服器GPU（Volta V100）有六個鏈路。而消費級GPU（RTX 2080Ti）只有一個鏈路，執行速度也降低到100Gbit/s。建議使用[NCCL](https://github.com/NVIDIA/nccl)來實現GPU之間的高速資料傳輸。

## 更多延遲

 :numref:`table_latency_numbers`和 :numref:`table_latency_numbers_tesla`中的小結來自[Eliot Eshelman](https://gist.github.com/eshelman)，他們將數字的更新版本儲存到[GitHub gist](https://gist.github.com/eshelman/343a1c46cb3fba142c1afdcdeec17646)。

:常見延遲。

| Action | Time | Notes |
| :----------------------------------------- | -----: | :---------------------------------------------- |
| L1 cache reference/hit                     | 1.5 ns | 4 cycles                                        |
| Floating-point add/mult/FMA                | 1.5 ns | 4 cycles                                        |
| L2 cache reference/hit                     |   5 ns | 12 ~ 17 cycles                                  |
| Branch mispredict                          |   6 ns | 15 ~ 20 cycles                                  |
| L3 cache hit (unshared cache)              |  16 ns | 42 cycles                                       |
| L3 cache hit (shared in another core)      |  25 ns | 65 cycles                                       |
| Mutex lock/unlock                          |  25 ns |                                                 |
| L3 cache hit (modified in another core)    |  29 ns | 75 cycles                                       |
| L3 cache hit (on a remote CPU socket)      |  40 ns | 100 ~ 300 cycles (40 ~ 116 ns)                  |
| QPI hop to a another CPU (per hop)         |  40 ns |                                                 |
| 64MB memory ref. (local CPU)          |  46 ns | TinyMemBench on Broadwell E5-2690v4             |
| 64MB memory ref. (remote CPU)         |  70 ns | TinyMemBench on Broadwell E5-2690v4             |
| 256MB memory ref. (local CPU)         |  75 ns | TinyMemBench on Broadwell E5-2690v4             |
| Intel Optane random write                  |  94 ns | UCSD Non-Volatile Systems Lab                   |
| 256MB memory ref. (remote CPU)        | 120 ns | TinyMemBench on Broadwell E5-2690v4             |
| Intel Optane random read                   | 305 ns | UCSD Non-Volatile Systems Lab                   |
| Send 4KB over 100 Gbps HPC fabric          |   1 μs | MVAPICH2 over Intel Omni-Path                   |
| Compress 1KB with Google Snappy            |   3 μs |                                                 |
| Send 4KB over 10 Gbps ethernet             |  10 μs |                                                 |
| Write 4KB randomly to NVMe SSD             |  30 μs | DC P3608 NVMe SSD (QOS 99% is 500μs)            |
| Transfer 1MB to/from NVLink GPU            |  30 μs | ~33GB/s on NVIDIA 40GB NVLink                 |
| Transfer 1MB to/from PCI-E GPU             |  80 μs | ~12GB/s on PCIe 3.0 x16 link                  |
| Read 4KB randomly from NVMe SSD            | 120 μs | DC P3608 NVMe SSD (QOS 99%)                     |
| Read 1MB sequentially from NVMe SSD        | 208 μs | ~4.8GB/s DC P3608 NVMe SSD                    |
| Write 4KB randomly to SATA SSD             | 500 μs | DC S3510 SATA SSD (QOS 99.9%)                   |
| Read 4KB randomly from SATA SSD            | 500 μs | DC S3510 SATA SSD (QOS 99.9%)                   |
| Round trip within same datacenter          | 500 μs | One-way ping is ~250μs                          |
| Read 1MB sequentially from SATA SSD        |   2 ms | ~550MB/s DC S3510 SATA SSD                    |
| Read 1MB sequentially from disk            |   5 ms | ~200MB/s server HDD                           |
| Random Disk Access (seek+rotation)         |  10 ms |                                                 |
| Send packet CA->Netherlands->CA            | 150 ms |                                                 |
:label:`table_latency_numbers`

:NVIDIA Tesla GPU的延遲.

| Action | Time | Notes |
| :------------------------------ | -----: | :---------------------------------------- |
| GPU Shared Memory access        |  30 ns | 30~90 cycles (bank conflicts add latency) |
| GPU Global Memory access        | 200 ns | 200~800 cycles                            |
| Launch CUDA kernel on GPU       |  10 μs | Host CPU instructs GPU to start kernel    |
| Transfer 1MB to/from NVLink GPU |  30 μs | ~33GB/s on NVIDIA 40GB NVLink           |
| Transfer 1MB to/from PCI-E GPU  |  80 μs | ~12GB/s on PCI-Express x16 link         |
:label:`table_latency_numbers_tesla`

## 小結

* 裝置有執行開銷。因此，資料傳輸要爭取量大次少而不是量少次多。這適用於RAM、固態驅動器、網路和GPU。
* 向量化是效能的關鍵。確保充分了解加速器的特定功能。例如，一些Intel Xeon CPU特別適用於INT8操作，NVIDIA Volta GPU擅長FP16矩陣操作，NVIDIA Turing擅長FP16、INT8和INT4操作。
* 在訓練過程中資料型別過小導致的數值溢位可能是個問題（在推斷過程中則影響不大）。
* 資料混疊現象會導致嚴重的效能退化。$64$位CPU應該按照$64$位邊界進行記憶體對齊。在GPU上建議保持卷積大小對齊，例如：與張量核對齊。
* 將演算法與硬體相匹配（例如，記憶體佔用和頻寬）。將命中引數裝入快取後，可以實現很大數量級的加速比。
* 在驗證實驗結果之前，建議先在紙上勾勒出新演算法的效能。關注的原因是數量級及以上的差異。
* 使用偵錯程式追蹤除錯尋找效能的瓶頸。
* 訓練硬體和推斷硬體在效能和價格方面有不同的優點。

## 練習

1. 編寫C語言來測試存取對齊的記憶體和未對齊的記憶體之間的速度是否有任何差異。（提示：小心快取影響。）
1. 測試按順序存取或按給定步幅存取記憶體時的速度差異。
1. 如何測量CPU上的快取大小？
1. 如何在多個記憶體通道中分配資料以獲得最大頻寬？如果有許多小的執行緒，會怎麼佈置？
1. 一個企業級硬碟正在以10000轉/分的速度旋轉。在最壞的情況下，硬碟讀取資料所需的最短時間是多少（假設磁頭幾乎是瞬間移動的）？為什麼2.5英寸硬碟在商用伺服器上越來越流行（相對於3.5英寸硬碟和5.25英寸硬碟）？
1. 假設HDD製造商將儲存密度從每平方英寸1 Tbit增加到每平方英寸5 Tbit。在一個2.5英寸的硬碟上，多少資訊能夠儲存一個環中？內軌和外軌有區別嗎？
1. 從$8$位資料型別到$16$位資料型別，矽片的數量大約增加了四倍，為什麼？為什麼NVIDIA會在其圖靈GPU中新增INT4運算？
1. 在記憶體中向前讀比向後讀快多少？該數字在不同的計算機和CPU供應商之間是否有所不同？為什麼？編寫C程式碼進行實驗。
1. 磁碟的快取大小能否測量？典型的硬碟是多少？固態驅動器需要快取嗎？
1. 測量透過乙太網傳送訊息時的資料包開銷。查詢UDP和TCP/IP連線之間的差異。
1. 直接記憶體存取允許CPU以外的裝置直接向記憶體寫入（和讀取）。為什麼要這樣？
1. 看看Turing T4GPU的效能數字。為什麼從FP16到INT8和INT4的效能只翻倍？
1. 一個網路包從舊金山到阿姆斯特丹的往返旅行需要多長時間？提示：可以假設距離為10000公里。

[Discussions](https://discuss.d2l.ai/t/5717)
