# 環境和分佈偏移

前面我們學習了許多機器學習的實際應用，將模型擬合各種資料集。
然而，我們從來沒有想過資料最初從哪裡來？以及我們計劃最終如何處理模型的輸出？
通常情況下，開發人員會擁有一些資料且急於開發模型，而不關注這些基本問題。

許多失敗的機器學習部署（即實際應用）都可以追究到這種方式。
有時，根據測試集的精度衡量，模型表現得非常出色。
但是當資料分佈突然改變時，模型在部署中會出現災難性的失敗。
更隱蔽的是，有時模型的部署本身就是擾亂資料分佈的催化劑。
舉一個有點荒謬卻可能真實存在的例子。
假設我們訓練了一個貸款申請人違約風險模型，用來預測誰將償還貸款或違約。
這個模型發現申請人的鞋子與違約風險相關（穿牛津鞋申請人會償還，穿運動鞋申請人會違約）。
此後，這個模型可能傾向於向所有穿著牛津鞋的申請人發放貸款，並拒絕所有穿著運動鞋的申請人。

這種情況可能會帶來災難性的後果。
首先，一旦模型開始根據鞋類做出決定，顧客就會理解並改變他們的行為。
不久，所有的申請者都會穿牛津鞋，而信用度卻沒有相應的提高。
總而言之，機器學習的許多應用中都存在類似的問題：
透過將基於模型的決策引入環境，我們可能會破壞模型。

雖然我們不可能在一節中討論全部的問題，但我們希望揭示一些常見的問題，
並激發批判性思考，以便及早發現這些情況，減輕災難性的損害。
有些解決方案很簡單（要求“正確”的資料），有些在技術上很困難（實施強化學習系統），
還有一些解決方案要求我們完全跳出統計預測，解決一些棘手的、與演算法倫理應用有關的哲學問題。

## 分佈偏移的型別

首先，我們考慮資料分佈可能發生變化的各種方式，以及為挽救模型效能可能採取的措施。
在一個經典的情景中，假設訓練資料是從某個分佈$p_S(\mathbf{x},y)$中取樣的，
但是測試資料將包含從不同分佈$p_T(\mathbf{x},y)$中抽取的未標記樣本。
一個清醒的現實是：如果沒有任何關於$p_S$和$p_T$之間相互關係的假設，
學習到一個分類器是不可能的。

考慮一個二元分類問題：區分狗和貓。
如果分佈可以以任意方式偏移，那麼我們的情景允許病態的情況，
即輸入的分佈保持不變：$p_S(\mathbf{x}) = p_T(\mathbf{x})$，
但標籤全部翻轉：$p_S(y | \mathbf{x}) = 1 - p_T(y | \mathbf{x})$。
換言之，如果將來所有的“貓”現在都是狗，而我們以前所說的“狗”現在是貓。
而此時輸入$p(\mathbf{x})$的分佈沒有任何改變，
那麼我們就不可能將這種情景與分佈完全沒有變化的情景區分開。

幸運的是，在對未來我們的資料可能發生變化的一些限制性假設下，
有些演算法可以檢測這種偏移，甚至可以動態調整，提高原始分類器的精度。

### 協變數偏移

在不同分佈偏移中，協變數偏移可能是最為廣泛研究的。
這裡我們假設：雖然輸入的分佈可能隨時間而改變，
但標籤函式（即條件分佈$P(y \mid \mathbf{x})$）沒有改變。
統計學家稱之為*協變數偏移*（covariate shift），
因為這個問題是由於協變數（特徵）分佈的變化而產生的。
雖然有時我們可以在不參考因果關係的情況下對分佈偏移進行推斷，
但在我們認為$\mathbf{x}$導致$y$的情況下，協變數偏移是一種自然假設。

考慮一下區分貓和狗的問題：訓練資料包括 :numref:`fig_cat-dog-train`中的圖像。

![區分貓和狗的訓練資料](../img/cat-dog-train.svg)
:label:`fig_cat-dog-train`

在測試時，我們被要求對 :numref:`fig_cat-dog-test`中的圖像進行分類別。

![區分貓和狗的測試資料](../img/cat-dog-test.svg)
:label:`fig_cat-dog-test`

訓練集由真實照片組成，而測試集只包含卡通圖片。
假設在一個與測試集的特徵有著本質不同的資料集上進行訓練，
如果沒有方法來適應新的領域，可能會有麻煩。

### 標籤偏移

*標籤偏移*（label shift）描述了與協變數偏移相反的問題。
這裡我們假設標籤邊緣機率$P(y)$可以改變，
但是類別條件分佈$P(\mathbf{x} \mid y)$在不同的領域之間保持不變。
當我們認為$y$導致$\mathbf{x}$時，標籤偏移是一個合理的假設。
例如，預測患者的疾病，我們可能根據症狀來判斷，
即使疾病的相對流行率隨著時間的推移而變化。
標籤偏移在這裡是恰當的假設，因為疾病會引起症狀。
在另一些情況下，標籤偏移和協變數偏移假設可以同時成立。
例如，當標籤是確定的，即使$y$導致$\mathbf{x}$，協變數偏移假設也會得到滿足。
有趣的是，在這些情況下，使用基於標籤偏移假設的方法通常是有利的。
這是因為這些方法傾向於包含看起來像標籤（通常是低維）的物件，
而不是像輸入（通常是高維的）物件。

### 概念偏移

我們也可能會遇到*概念偏移*（concept shift）：
當標籤的定義發生變化時，就會出現這種問題。
這聽起來很奇怪——一隻貓就是一隻貓，不是嗎？
然而，其他類別會隨著不同時間的用法而發生變化。
精神疾病的診斷標準、所謂的時髦、以及工作頭銜等等，都是概念偏移的日常對映。
事實證明，假如我們環遊美國，根據所在的地理位置改變我們的資料來源，
我們會發現關於“軟飲”名稱的分佈發生了相當大的概念偏移，
如 :numref:`fig_popvssoda` 所示。

![美國軟飲名稱的概念偏移](../img/popvssoda.png)
:width:`400px`
:label:`fig_popvssoda`

如果我們要建立一個機器翻譯系統，
$P(y \mid \mathbf{x})$的分佈可能會因我們的位置不同而得到不同的翻譯。
這個問題可能很難被發現。
所以，我們最好可以利用在時間或空間上逐漸發生偏移的知識。

## 分佈偏移範例

在深入研究形式體系和演算法之前，我們可以討論一些協變數偏移或概念偏移可能並不明顯的具體情況。

### 醫學診斷

假設我們想設計一個檢測癌症的演算法，從健康人和病人那裡收集資料，然後訓練演算法。
它工作得很好，有很高的精度，然後我們得出了已經準備好在醫療診斷上取得成功的結論。
請先彆著急。

收集訓練資料的分佈和在實際中遇到的資料分佈可能有很大的不同。
這件事在一個不幸的初創公司身上發生過，我們中的一些作者幾年前和他們合作過。
他們正在研究一種血液檢測方法，主要針對一種影響老年男性的疾病，
並希望利用他們從病人身上採集的血液樣本進行研究。
然而，從健康男性身上獲取血樣比從系統中已有的病人身上獲取要困難得多。
作為補償，這家初創公司向一所大學校園內的學生徵集獻血，作為開發測試的健康對照樣本。
然後這家初創公司問我們是否可以幫助他們建立一個用於檢測疾病的分類器。

正如我們向他們解釋的那樣，用近乎完美的精度來區分健康和患病人群確實很容易。
然而，這可能是因為受試者在年齡、激素水平、體力活動、
飲食、飲酒以及其他許多與疾病無關的因素上存在差異。
這對檢測疾病的分類器可能並不適用。
這些抽樣可能會遇到極端的協變數偏移。
此外，這種情況不太可能透過常規方法加以糾正。
簡言之，他們浪費了一大筆錢。

### 自動駕駛汽車

對於一家想利用機器學習來開發自動駕駛汽車的公司，一個關鍵部件是“路沿檢測器”。
由於真實的註釋資料獲取成本很高，他們想出了一個“聰明”的想法：
將遊戲渲染引擎中的合成數據用作額外的訓練資料。
這對從渲染引擎中抽取的“測試資料”非常有效，但應用在一輛真正的汽車裡真是一場災難。
正如事實證明的那樣，路沿被渲染成一種非常簡單的紋理。
更重要的是，所有的路沿都被渲染成了相同的紋理，路沿檢測器很快就學習到了這個“特徵”。

當美軍第一次試圖在森林中探測坦克時，也發生了類似的事情。
他們在沒有坦克的情況下拍攝了森林的航拍照片，然後把坦克開進森林，拍攝了另一組照片。
使用這兩組資料訓練的分類器似乎工作得很好。
不幸的是，分類器僅僅學會了如何區分有陰影的樹和沒有陰影的樹：
第一組照片是在清晨拍攝的，而第二組是在中午拍攝的。

### 非平穩分佈

當分佈變化緩慢並且模型沒有得到充分更新時，就會出現更微妙的情況：
*非平穩分佈*（nonstationary distribution）。
以下是一些典型例子：

* 訓練一個計算廣告模型，但卻沒有經常更新（例如，一個2009年訓練的模型不知道一個叫iPad的不知名新裝置剛剛上市）；
* 建立一個垃圾郵件過濾器，它能很好地檢測到所有垃圾郵件。但是，垃圾郵件傳送者們變得聰明起來，製造出新的資訊，看起來不像我們以前見過的任何垃圾郵件；
* 建立一個產品推薦系統，它在整個冬天都有效，但聖誕節過後很久還會繼續推薦聖誕帽。

### 更多軼事

* 建立一個人臉檢測器，它在所有基準測試中都能很好地工作，但是它在測試資料上失敗了：有問題的例子是人臉充滿了整個圖像的特寫鏡頭（訓練集中沒有這樣的資料）。
* 為美國市場建立了一個網路搜尋引擎，並希望將其部署到英國。
* 透過在一個大的資料集來訓練圖像分類器，其中每一個大類別的數量在資料集近乎是平均的，比如1000個類別，每個類別由1000個圖像表示。但是將該系統部署到真實世界中，照片的實際標籤分佈顯然是不均勻的。

## 分佈偏移糾正

正如我們所討論的，在許多情況下訓練和測試分佈$P(\mathbf{x}, y)$是不同的。
在一些情況下，我們很幸運，不管協變數、標籤或概念如何發生偏移，模型都能正常工作。
在另一些情況下，我們可以透過運用策略來應對這種偏移，從而做得更好。
本節的其餘部分將著重於應對這種偏移的技術細節。

### 經驗風險與實際風險
:label:`subsec_empirical-risk-and-risk`

首先我們反思一下在模型訓練期間到底發生了什麼？
訓練資料$\{(\mathbf{x}_1, y_1), \ldots, (\mathbf{x}_n, y_n)\}$
的特徵和相關的標籤經過迭代，在每一個小批次之後更新模型$f$的引數。
為了簡單起見，我們不考慮正則化，因此極大地降低了訓練損失：

$$\mathop{\mathrm{minimize}}_f \frac{1}{n} \sum_{i=1}^n l(f(\mathbf{x}_i), y_i),$$
:eqlabel:`eq_empirical-risk-min`

其中$l$是損失函式，用來度量：
給定標籤$y_i$，預測$f(\mathbf{x}_i)$的“糟糕程度”。
統計學家稱 :eqref:`eq_empirical-risk-min`中的這一項為經驗風險。
*經驗風險*（empirical risk）是為了近似 *真實風險*（true risk），
整個訓練資料上的平均損失，即從其真實分佈$p(\mathbf{x},y)$中
抽取的所有資料的總體損失的期望值：

$$E_{p(\mathbf{x}, y)} [l(f(\mathbf{x}), y)] = \int\int l(f(\mathbf{x}), y) p(\mathbf{x}, y) \;d\mathbf{x}dy.$$
:eqlabel:`eq_true-risk`

然而在實踐中，我們通常無法獲得總體資料。
因此，*經驗風險最小化*即在 :eqref:`eq_empirical-risk-min`中最小化經驗風險，
是一種實用的機器學習策略，希望能近似最小化真實風險。

### 協變數偏移糾正
:label:`subsec_covariate-shift-correction`

假設對於帶標籤的資料$(\mathbf{x}_i, y_i)$，
我們要評估$P(y \mid \mathbf{x})$。
然而觀測值$\mathbf{x}_i$是從某些*源分佈*$q(\mathbf{x})$中得出的，
而不是從*目標分佈*$p(\mathbf{x})$中得出的。
幸運的是，依賴性假設意味著條件分佈保持不變，即：
$p(y \mid \mathbf{x}) = q(y \mid \mathbf{x})$。
如果源分佈$q(\mathbf{x})$是“錯誤的”，
我們可以透過在真實風險的計算中，使用以下簡單的恆等式來進行糾正：

$$
\begin{aligned}
\int\int l(f(\mathbf{x}), y) p(y \mid \mathbf{x})p(\mathbf{x}) \;d\mathbf{x}dy =
\int\int l(f(\mathbf{x}), y) q(y \mid \mathbf{x})q(\mathbf{x})\frac{p(\mathbf{x})}{q(\mathbf{x})} \;d\mathbf{x}dy.
\end{aligned}
$$

換句話說，我們需要根據資料來自正確分佈與來自錯誤分佈的機率之比，
來重新衡量每個資料樣本的權重：

$$\beta_i \stackrel{\mathrm{def}}{=} \frac{p(\mathbf{x}_i)}{q(\mathbf{x}_i)}.$$

將權重$\beta_i$代入到每個資料樣本$(\mathbf{x}_i, y_i)$中，
我們可以使用”加權經驗風險最小化“來訓練模型：

$$\mathop{\mathrm{minimize}}_f \frac{1}{n} \sum_{i=1}^n \beta_i l(f(\mathbf{x}_i), y_i).$$
:eqlabel:`eq_weighted-empirical-risk-min`

由於不知道這個比率，我們需要估計它。
有許多方法都可以用，包括一些花哨的運算元理論方法，
試圖直接使用最小范數或最大熵原理重新校準期望運算元。
對於任意一種這樣的方法，我們都需要從兩個分佈中抽取樣本：
“真實”的分佈$p$，透過存取測試資料獲取；
訓練集$q$，透過人工合成的很容易獲得。
請注意，我們只需要特徵$\mathbf{x} \sim p(\mathbf{x})$，
不需要存取標籤$y \sim p(y)$。

在這種情況下，有一種非常有效的方法可以得到幾乎與原始方法一樣好的結果：
*對數機率迴歸*（logistic regression）。
這是用於二元分類別的softmax迴歸（見 :numref:`sec_softmax`）的一個特例。
綜上所述，我們學習了一個分類器來區分從$p(\mathbf{x})$抽取的資料
和從$q(\mathbf{x})$抽取的資料。
如果無法區分這兩個分佈，則意味著相關的樣本可能來自這兩個分佈中的任何一個。
另一方面，任何可以很好區分的樣本都應該相應地顯著增加或減少權重。

為了簡單起見，假設我們分別從$p(\mathbf{x})$和$q(\mathbf{x})$
兩個分佈中抽取相同數量的樣本。
現在用$z$標籤表示：從$p$抽取的資料為$1$，從$q$抽取的資料為$-1$。
然後，混合資料集中的機率由下式給出

$$P(z=1 \mid \mathbf{x}) = \frac{p(\mathbf{x})}{p(\mathbf{x})+q(\mathbf{x})} \text{ and hence } \frac{P(z=1 \mid \mathbf{x})}{P(z=-1 \mid \mathbf{x})} = \frac{p(\mathbf{x})}{q(\mathbf{x})}.$$

因此，如果我們使用對數機率迴歸方法，其中
$P(z=1 \mid \mathbf{x})=\frac{1}{1+\exp(-h(\mathbf{x}))}$
（$h$是一個引數化函式），則很自然有：

$$
\beta_i = \frac{1/(1 + \exp(-h(\mathbf{x}_i)))}{\exp(-h(\mathbf{x}_i))/(1 + \exp(-h(\mathbf{x}_i)))} = \exp(h(\mathbf{x}_i)).
$$

因此，我們需要解決兩個問題：
第一個問題是關於區分來自兩個分佈的資料；
第二個問題是關於 :eqref:`eq_weighted-empirical-risk-min`
中的加權經驗風險的最小化問題。
在這個問題中，我們將對其中的項加權$\beta_i$。

現在，我們來看一下完整的協變數偏移糾正演算法。
假設我們有一個訓練集$\{(\mathbf{x}_1, y_1), \ldots, (\mathbf{x}_n, y_n)\}$
和一個未標記的測試集$\{\mathbf{u}_1, \ldots, \mathbf{u}_m\}$。
對於協變數偏移，我們假設$1 \leq i \leq n$的$\mathbf{x}_i$來自某個源分佈，
$\mathbf{u}_i$來自目標分佈。
以下是糾正協變數偏移的典型演算法：

1. 產生一個二元分類訓練集：$\{(\mathbf{x}_1, -1), \ldots, (\mathbf{x}_n, -1), (\mathbf{u}_1, 1), \ldots, (\mathbf{u}_m, 1)\}$。
1. 用對數機率迴歸訓練二元分類器得到函式$h$。
1. 使用$\beta_i = \exp(h(\mathbf{x}_i))$或更好的$\beta_i = \min(\exp(h(\mathbf{x}_i)), c)$（$c$為常量）對訓練資料進行加權。
1. 使用權重$\beta_i$進行 :eqref:`eq_weighted-empirical-risk-min` 中$\{(\mathbf{x}_1, y_1), \ldots, (\mathbf{x}_n, y_n)\}$的訓練。

請注意，上述演算法依賴於一個重要的假設：
需要目標分佈(例如，測試分佈)中的每個資料樣本在訓練時出現的機率非零。
如果我們找到$p(\mathbf{x}) > 0$但$q(\mathbf{x}) = 0$的點，
那麼相應的重要性權重會是無窮大。

### 標籤偏移糾正

假設我們處理的是$k$個類別的分類任務。
使用 :numref:`subsec_covariate-shift-correction`中相同符號，
$q$和$p$中分別是源分佈（例如訓練時的分佈）和目標分佈（例如測試時的分佈）。
假設標籤的分佈隨時間變化：$q(y) \neq p(y)$，
但類別條件分佈保持不變：$q(\mathbf{x} \mid y)=p(\mathbf{x} \mid y)$。
如果源分佈$q(y)$是“錯誤的”，
我們可以根據 :eqref:`eq_true-risk`中定義的真實風險中的恆等式進行更正：

$$
\begin{aligned}
\int\int l(f(\mathbf{x}), y) p(\mathbf{x} \mid y)p(y) \;d\mathbf{x}dy =
\int\int l(f(\mathbf{x}), y) q(\mathbf{x} \mid y)q(y)\frac{p(y)}{q(y)} \;d\mathbf{x}dy.
\end{aligned}
$$

這裡，重要性權重將對應於標籤似然比率

$$\beta_i \stackrel{\mathrm{def}}{=} \frac{p(y_i)}{q(y_i)}.$$

標籤偏移的一個好處是，如果我們在源分佈上有一個相當好的模型，
那麼我們可以得到對這些權重的一致估計，而不需要處理周邊的其他維度。
在深度學習中，輸入往往是高維物件（如圖像），而標籤通常是低維（如類別）。

為了估計目標標籤分佈，我們首先採用效能相當好的現成的分類器（通常基於訓練資料進行訓練），
並使用驗證集（也來自訓練分佈）計算其混淆矩陣。
混淆矩陣$\mathbf{C}$是一個$k \times k$矩陣，
其中每列對應於標籤類別，每行對應於模型的預測類別。
每個單元格的值$c_{ij}$是驗證集中，真實標籤為$j$，
而我們的模型預測為$i$的樣本數量所佔的比例。

現在，我們不能直接計算目標資料上的混淆矩陣，
因為我們無法看到真實環境下的樣本的標籤，
除非我們再搭建一個複雜的即時標註流程。
然而，我們所能做的是將所有模型在測試時的預測取平均數，
得到平均模型輸出$\mu(\hat{\mathbf{y}}) \in \mathbb{R}^k$，
其中第$i$個元素$\mu(\hat{y}_i)$是我們模型預測測試集中$i$的總預測分數。

結果表明，如果我們的分類器一開始就相當準確，
並且目標資料只包含我們以前見過的類別，
以及如果標籤偏移假設成立（這裡最強的假設），
我們就可以透過求解一個簡單的線性系統來估計測試集的標籤分佈

$$\mathbf{C} p(\mathbf{y}) = \mu(\hat{\mathbf{y}}),$$

因為作為一個估計，$\sum_{j=1}^k c_{ij} p(y_j) = \mu(\hat{y}_i)$
對所有$1 \leq i \leq k$成立，
其中$p(y_j)$是$k$維標籤分佈向量$p(\mathbf{y})$的第$j^\mathrm{th}$元素。
如果我們的分類器一開始就足夠精確，那麼混淆矩陣$\mathbf{C}$將是可逆的，
進而我們可以得到一個解$p(\mathbf{y}) = \mathbf{C}^{-1} \mu(\hat{\mathbf{y}})$。

因為我們觀測源資料上的標籤，所以很容易估計分佈$q(y)$。
那麼對於標籤為$y_i$的任何訓練樣本$i$，
我們可以使用我們估計的$p(y_i)/q(y_i)$比率來計算權重$\beta_i$，
並將其代入 :eqref:`eq_weighted-empirical-risk-min`中的加權經驗風險最小化中。

### 概念偏移糾正

概念偏移很難用原則性的方式解決。
例如，在一個問題突然從“區分貓和狗”偏移為“區分白色和黑色動物”的情況下，
除了從零開始收集新標籤和訓練，別無妙方。
幸運的是，在實踐中這種極端的偏移是罕見的。
相反，通常情況下，概念的變化總是緩慢的。
比如下面是一些例子：

* 在計算廣告中，新產品推出後，舊產品變得不那麼受歡迎了。這意味著廣告的分佈和受歡迎程度是逐漸變化的，任何點選率預測器都需要隨之逐漸變化；
* 由於環境的磨損，交通攝影頭的鏡頭會逐漸退化，影響攝影頭的圖像品質；
* 新聞內容逐漸變化（即新新聞的出現）。

在這種情況下，我們可以使用與訓練網路相同的方法，使其適應資料的變化。
換言之，我們使用新資料更新現有的網路權重，而不是從頭開始訓練。

## 學習問題的分類法

有了如何處理分佈變化的知識，我們現在可以考慮機器學習問題形式化的其他方面。

### 批次學習

在*批次學習*（batch learning）中，我們可以存取一組訓練特徵和標籤
$\{(\mathbf{x}_1, y_1), \ldots, (\mathbf{x}_n, y_n)\}$，
我們使用這些特性和標籤訓練$f(\mathbf{x})$。
然後，我們部署此模型來對來自同一分佈的新資料$(\mathbf{x}, y)$進行評分。
例如，我們可以根據貓和狗的大量圖片訓練貓檢測器。
一旦我們訓練了它，我們就把它作為智慧貓門計算視覺系統的一部分，來控制只允許貓進入。
然後這個系統會被安裝在客戶家中，基本再也不會更新。

### 線上學習

除了“批次”地學習，我們還可以單個“線上”學習資料$(\mathbf{x}_i, y_i)$。
更具體地說，我們首先觀測到$\mathbf{x}_i$，
然後我們得出一個估計值$f(\mathbf{x}_i)$，
只有當我們做到這一點後，我們才觀測到$y_i$。
然後根據我們的決定，我們會得到獎勵或損失。
許多實際問題都屬於這一類別。
例如，我們需要預測明天的股票價格，
這樣我們就可以根據這個預測進行交易。
在一天結束時，我們會評估我們的預測是否盈利。
換句話說，在*線上學習*（online learning）中，我們有以下的迴圈。
在這個迴圈中，給定新的觀測結果，我們會不斷地改進我們的模型。

$$
\mathrm{model} ~ f_t \longrightarrow
\mathrm{data} ~ \mathbf{x}_t \longrightarrow
\mathrm{estimate} ~ f_t(\mathbf{x}_t) \longrightarrow
\mathrm{observation} ~ y_t \longrightarrow
\mathrm{loss} ~ l(y_t, f_t(\mathbf{x}_t)) \longrightarrow
\mathrm{model} ~ f_{t+1}
$$

### 老虎機

*老虎機*（bandits）是上述問題的一個特例。
雖然在大多數學習問題中，我們有一個連續引數化的函式$f$（例如，一個深度網路）。
但在一個*老虎機*問題中，我們只有有限數量的手臂可以拉動。
也就是說，我們可以採取的行動是有限的。
對於這個更簡單的問題，可以獲得更強的最優性理論保證，這並不令人驚訝。
我們之所以列出它，主要是因為這個問題經常被視為一個單獨的學習問題的情景。

### 控制

在很多情況下，環境會記住我們所做的事。
不一定是以一種對抗的方式，但它會記住，而且它的反應將取決於之前發生的事情。
例如，咖啡鍋爐控制器將根據之前是否加熱鍋爐來觀測到不同的溫度。
在這種情況下，PID（比例—積分—微分）控制器演算法是一個流行的選擇。
同樣，一個使用者在新聞網站上的行為將取決於之前向她展示的內容（例如，大多數新聞她只閱讀一次）。
許多這樣的演算法形成了一個環境模型，在這個模型中，他們的行為使得他們的決策看起來不那麼隨機。
近年來，控制理論（如PID的變體）也被用於自動調整超引數，
以獲得更好的解構和重建品質，提高產生文字的多樣性和產生圖像的重建品質
 :cite:`Shao.Yao.Sun.ea.2020`。

### 強化學習

*強化學習*（reinforcement learning）強調如何基於環境而行動，以取得最大化的預期利益。
國際象棋、圍棋、西洋雙陸棋或星際爭霸都是強化學習的應用例項。
再比如，為自動駕駛汽車製造一個控制器，或者以其他方式對自動駕駛汽車的駕駛方式做出反應
（例如，試圖避開某物體，試圖造成事故，或者試圖與其合作）。

### 考慮到環境

上述不同情況之間的一個關鍵區別是：
在靜止環境中可能一直有效的相同策略，
在環境能夠改變的情況下可能不會始終有效。
例如，一個交易者發現的套利機會很可能在他開始利用它時就消失了。
環境變化的速度和方式在很大程度上決定了我們可以採用的演算法型別。
例如，如果我們知道事情只會緩慢地變化，
就可以迫使任何估計也只能緩慢地發生改變。
如果我們知道環境可能會瞬間發生變化，但這種變化非常罕見，
我們就可以在使用演算法時考慮到這一點。
當一個數據科學家試圖解決的問題會隨著時間的推移而發生變化時，
這些型別的知識至關重要。

## 機器學習中的公平、責任和透明度

最後，重要的是，當我們部署機器學習系統時，
不僅僅是在最佳化一個預測模型，
而通常是在提供一個會被用來（部分或完全）進行自動化決策的工具。
這些技術系統可能會透過其進行的決定而影響到每個人的生活。

從考慮預測到決策的飛躍不僅提出了新的技術問題，
而且還提出了一系列必須仔細考慮的倫理問題。
如果我們正在部署一個醫療診斷系統，我們需要知道它可能適用於哪些人群，哪些人群可能無效。
忽視對一個亞群體的幸福的可預見風險可能會導致我們執行劣質的護理水平。
此外，一旦我們規劃整個決策系統，我們必須退後一步，重新考慮如何評估我們的技術。
在這個視野變化所導致的結果中，我們會發現精度很少成為合適的衡量標準。
例如，當我們將預測轉化為行動時，我們通常會考慮到各種方式犯錯的潛在成本敏感性。
舉個例子：將圖像錯誤地分到某一類別可能被視為種族歧視，而錯誤地分到另一個類別是無害的，
那麼我們可能需要相應地調整我們的閾值，在設計決策方式時考慮到這些社會價值。
我們還需要注意預測系統如何導致反饋迴圈。
例如，考慮預測性警務系統，它將巡邏人員分配到預測犯罪率較高的地區。
很容易看出一種令人擔憂的模式是如何出現的：

 1. 犯罪率高的社群會得到更多的巡邏；
 2. 因此，在這些社群中會發現更多的犯罪行為，輸入可用於未來迭代的訓練資料；
 3. 面對更多的積極因素，該模型預測這些社群還會有更多的犯罪；
 4. 下一次迭代中，更新後的模型會更加傾向於針對同一個地區，這會導致更多的犯罪行為被發現等等。

通常，在建模糾正過程中，模型的預測與訓練資料耦合的各種機制都沒有得到解釋，
研究人員稱之為“失控反饋迴圈”的現象。
此外，我們首先要注意我們是否解決了正確的問題。
比如，預測演算法現在在資訊傳播中起著巨大的中介作用，
個人看到的新聞應該由他們喜歡的Facebook頁面決定嗎？
這些只是在機器學習職業生涯中可能遇到的令人感到“壓力山大”的道德困境中的一小部分。

## 小結

* 在許多情況下，訓練集和測試集並不來自同一個分佈。這就是所謂的分佈偏移。
* 真實風險是從真實分佈中抽取的所有資料的總體損失的預期。然而，這個資料總體通常是無法獲得的。經驗風險是訓練資料的平均損失，用於近似真實風險。在實踐中，我們進行經驗風險最小化。
* 在相應的假設條件下，可以在測試時檢測並糾正協變數偏移和標籤偏移。在測試時，不考慮這種偏移可能會成為問題。
* 在某些情況下，環境可能會記住自動操作並以令人驚訝的方式做出響應。在建構模型時，我們必須考慮到這種可能性，並繼續監控即時系統，並對我們的模型和環境以意想不到的方式糾纏在一起的可能性持開放態度。

## 練習

1. 當我們改變搜尋引擎的行為時會發生什麼？使用者可能會做什麼？廣告商呢？
2. 實現一個協變數偏移檢測器。提示：建構一個分類器。
3. 實現協變數偏移糾正。
4. 除了分佈偏移，還有什麼會影響經驗風險接近真實風險的程度？

[Discussions](https://discuss.d2l.ai/t/1822)
