# 現代迴圈神經網路
:label:`chap_modern_rnn`

前一章中我們介紹了迴圈神經網路的基礎知識，
這種網路可以更好地處理序列資料。
我們在文字資料上實現了基於迴圈神經網路的語言模型，
但是對於當今各種各樣的序列學習問題，這些技術可能並不夠用。

例如，迴圈神經網路在實踐中一個常見問題是數值不穩定性。
儘管我們已經應用了梯度裁剪等技巧來緩解這個問題，
但是仍需要透過設計更復雜的序列模型來進一步處理它。
具體來說，我們將引入兩個廣泛使用的網路，
即*門控迴圈單元*（gated recurrent units，GRU）和
*長短期記憶網路*（long short-term memory，LSTM）。
然後，我們將基於一個單向隱藏層來擴充迴圈神經網路架構。
我們將描述具有多個隱藏層的深層架構，
並討論基於前向和後向迴圈計算的雙向設計。
現代迴圈網路經常採用這種擴充。
在解釋這些迴圈神經網路的變體時，
我們將繼續考慮 :numref:`chap_rnn`中的語言建模問題。

事實上，語言建模只揭示了序列學習能力的冰山一角。
在各種序列學習問題中，如自動語音識別、文字到語音轉換和機器翻譯，
輸入和輸出都是任意長度的序列。
為了闡述如何擬合這種型別的資料，
我們將以機器翻譯為例介紹基於迴圈神經網路的
“編碼器－解碼器”架構和束搜尋，並用它們來產生序列。

```toc
:maxdepth: 2

gru
lstm
deep-rnn
bi-rnn
machine-translation-and-dataset
encoder-decoder
seq2seq
beam-search
```
