# 最佳化演算法
:label:`chap_optimization`

截止到目前，本書已經使用了許多最佳化演算法來訓練深度學習模型。最佳化演算法使我們能夠繼續更新模型引數，並使損失函式的值最小化。這就像在訓練集上評估一樣。事實上，任何滿足於將最佳化視為黑盒裝置，以在簡單的設定中最小化目標函式的人，都可能會知道存在著一系列此類“咒語”（名稱如“SGD”和“Adam”）。

但是，為了做得更好，還需要更深入的知識。最佳化演算法對於深度學習非常重要。一方面，訓練複雜的深度學習模型可能需要數小時、幾天甚至數週。最佳化演算法的效能直接影響模型的訓練效率。另一方面，瞭解不同最佳化演算法的原則及其超引數的作用將使我們能夠以有針對性的方式調整超引數，以提高深度學習模型的效能。

在本章中，我們深入探討常見的深度學習最佳化演算法。深度學習中出現的幾乎所有最佳化問題都是*非凸*的。儘管如此，在*凸問題*背景下設計和分析演算法是非常有啟發性的。正是出於這個原因，本章包括了凸最佳化的入門，以及凸目標函式上非常簡單的隨機梯度下降演算法的證明。

```toc
:maxdepth: 2

optimization-intro
convexity
gd
sgd
minibatch-sgd
momentum
adagrad
rmsprop
adadelta
adam
lr-scheduler
```
