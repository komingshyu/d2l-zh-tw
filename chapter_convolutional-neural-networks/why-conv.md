# 從全連線層到卷積
:label:`sec_why-conv`

我們之前討論的多層感知機十分適合處理表格資料，其中行對應樣本，列對應特徵。
對於表格資料，我們尋找的模式可能涉及特徵之間的互動，但是我們不能預先假設任何與特徵互動相關的先驗結構。
此時，多層感知機可能是最好的選擇，然而對於高維感知資料，這種缺少結構的網路可能會變得不實用。

例如，在之前貓狗分類別的例子中：假設我們有一個足夠充分的照片資料集，資料集中是擁有標註的照片，每張照片具有百萬級畫素，這意味著網路的每次輸入都有一百萬個維度。
即使將隱藏層維度降低到1000，這個全連線層也將有$10^6 \times 10^3 = 10^9$個引數。
想要訓練這個模型將不可實現，因為需要有大量的GPU、分散式最佳化訓練的經驗和超乎常人的耐心。

有些讀者可能會反對這個觀點，認為要求百萬畫素的解析度可能不是必要的。
然而，即使解析度減小為十萬畫素，使用1000個隱藏單元的隱藏層也可能不足以學習到良好的圖像特徵，在真實的系統中我們仍然需要數十億個引數。
此外，擬合如此多的引數還需要收集大量的資料。
然而，如今人類和機器都能很好地區分貓和狗：這是因為圖像中本就擁有豐富的結構，而這些結構可以被人類和機器學習模型使用。
*卷積神經網路*（convolutional neural networks，CNN）是機器學習利用自然圖像中一些已知結構的創造性方法。

## 不變性

想象一下，假設我們想從一張圖片中找到某個物體。
合理的假設是：無論哪種方法找到這個物體，都應該和物體的位置無關。
理想情況下，我們的系統應該能夠利用常識：豬通常不在天上飛，飛機通常不在水裡游泳。
但是，如果一隻豬出現在圖片頂部，我們還是應該認出它。
我們可以從兒童遊戲”沃爾多在哪裡”（ :numref:`img_waldo`）中得到靈感：
在這個遊戲中包含了許多充斥著活動的混亂場景，而沃爾多通常潛伏在一些不太可能的位置，讀者的目標就是找出他。
儘管沃爾多的裝扮很有特點，但是在眼花繚亂的場景中找到他也如大海撈針。
然而沃爾多的樣子並不取決於他潛藏的地方，因此我們可以使用一個“沃爾多檢測器”掃描圖像。
該檢測器將圖像分割成多個區域，併為每個區域包含沃爾多的可能性打分。
卷積神經網路正是將*空間不變性*（spatial invariance）的這一概念系統化，從而基於這個模型使用較少的引數來學習有用的表示。

![沃爾多遊戲範例圖。](../img/where-wally-walker-books.jpg)
:width:`400px`
:label:`img_waldo`

現在，我們將上述想法總結一下，從而幫助我們設計適合於計算機視覺的神經網路架構。

1. *平移不變性*（translation invariance）：不管檢測物件出現在圖像中的哪個位置，神經網路的前面幾層應該對相同的圖像區域具有相似的反應，即為“平移不變性”。
1. *區域性性*（locality）：神經網路的前面幾層應該只探索輸入圖像中的區域性區域，而不過度在意圖像中相隔較遠區域的關係，這就是“區域性性”原則。最終，可以聚合這些區域性特徵，以在整個圖像級別進行預測。

讓我們看看這些原則是如何轉化為數學表示的。

## 多層感知機的限制

首先，多層感知機的輸入是二維圖像$\mathbf{X}$，其隱藏表示$\mathbf{H}$在數學上是一個矩陣，在程式碼中表示為二維張量。
其中$\mathbf{X}$和$\mathbf{H}$具有相同的形狀。
為了方便理解，我們可以認為，無論是輸入還是隱藏表示都擁有空間結構。

使用$[\mathbf{X}]_{i, j}$和$[\mathbf{H}]_{i, j}$分別表示輸入圖像和隱藏表示中位置（$i$,$j$）處的畫素。
為了使每個隱藏神經元都能接收到每個輸入畫素的資訊，我們將引數從權重矩陣（如同我們先前在多層感知機中所做的那樣）替換為四階權重張量$\mathsf{W}$。假設$\mathbf{U}$包含偏置引數，我們可以將全連線層形式化地表示為

$$\begin{aligned} \left[\mathbf{H}\right]_{i, j} &= [\mathbf{U}]_{i, j} + \sum_k \sum_l[\mathsf{W}]_{i, j, k, l}  [\mathbf{X}]_{k, l}\\ &=  [\mathbf{U}]_{i, j} +
\sum_a \sum_b [\mathsf{V}]_{i, j, a, b}  [\mathbf{X}]_{i+a, j+b}.\end{aligned}$$

其中，從$\mathsf{W}$到$\mathsf{V}$的轉換隻是形式上的轉換，因為在這兩個四階張量的元素之間存在一一對應的關係。
我們只需重新索引下標$(k, l)$，使$k = i+a$、$l = j+b$，由此可得$[\mathsf{V}]_{i, j, a, b} = [\mathsf{W}]_{i, j, i+a, j+b}$。
索引$a$和$b$透過在正偏移和負偏移之間移動覆蓋了整個圖像。
對於隱藏表示中任意給定位置（$i$,$j$）處的畫素值$[\mathbf{H}]_{i, j}$，可以透過在$x$中以$(i, j)$為中心對畫素進行加權求和得到，加權使用的權重為$[\mathsf{V}]_{i, j, a, b}$。

### 平移不變性

現在參考上述的第一個原則：平移不變性。
這意味著檢測物件在輸入$\mathbf{X}$中的平移，應該僅導致隱藏表示$\mathbf{H}$中的平移。也就是說，$\mathsf{V}$和$\mathbf{U}$實際上不依賴於$(i, j)$的值，即$[\mathsf{V}]_{i, j, a, b} = [\mathbf{V}]_{a, b}$。並且$\mathbf{U}$是一個常數，比如$u$。因此，我們可以簡化$\mathbf{H}$定義為：

$$[\mathbf{H}]_{i, j} = u + \sum_a\sum_b [\mathbf{V}]_{a, b} [\mathbf{X}]_{i+a, j+b}.$$

這就是*卷積*（convolution）。我們是在使用係數$[\mathbf{V}]_{a, b}$對位置$(i, j)$附近的畫素$(i+a, j+b)$進行加權得到$[\mathbf{H}]_{i, j}$。
注意，$[\mathbf{V}]_{a, b}$的係數比$[\mathsf{V}]_{i, j, a, b}$少很多，因為前者不再依賴於圖像中的位置。這就是顯著的進步！

### 區域性性

現在參考上述的第二個原則：區域性性。如上所述，為了收集用來訓練引數$[\mathbf{H}]_{i, j}$的相關資訊，我們不應偏離到距$(i, j)$很遠的地方。這意味著在$|a|> \Delta$或$|b| > \Delta$的範圍之外，我們可以設定$[\mathbf{V}]_{a, b} = 0$。因此，我們可以將$[\mathbf{H}]_{i, j}$重寫為

$$[\mathbf{H}]_{i, j} = u + \sum_{a = -\Delta}^{\Delta} \sum_{b = -\Delta}^{\Delta} [\mathbf{V}]_{a, b}  [\mathbf{X}]_{i+a, j+b}.$$
:eqlabel:`eq_conv-layer`

簡而言之， :eqref:`eq_conv-layer`是一個*卷積層*（convolutional layer），而卷積神經網路是包含卷積層的一類特殊的神經網路。
在深度學習研究社群中，$\mathbf{V}$被稱為*卷積核*（convolution kernel）或者*濾波器*（filter），亦或簡單地稱之為該卷積層的*權重*，通常該權重是可學習的引數。
當圖像處理的區域性區域很小時，卷積神經網路與多層感知機的訓練差異可能是巨大的：以前，多層感知機可能需要數十億個引數來表示網路中的一層，而現在卷積神經網路通常只需要幾百個引數，而且不需要改變輸入或隱藏表示的維數。
引數大幅減少的代價是，我們的特徵現在是平移不變的，並且當確定每個隱藏活性值時，每一層只包含區域性的資訊。
以上所有的權重學習都將依賴於歸納偏置。當這種偏置與現實相符時，我們就能得到樣本有效的模型，並且這些模型能很好地泛化到未知資料中。
但如果這偏置與現實不符時，比如當圖像不滿足平移不變時，我們的模型可能難以擬合我們的訓練資料。

## 卷積

在進一步討論之前，我們先簡要回顧一下為什麼上面的操作被稱為卷積。在數學中，兩個函式（比如$f, g: \mathbb{R}^d \to \mathbb{R}$）之間的“卷積”被定義為

$$(f * g)(\mathbf{x}) = \int f(\mathbf{z}) g(\mathbf{x}-\mathbf{z}) d\mathbf{z}.$$

也就是說，卷積是當把一個函式“翻轉”並移位$\mathbf{x}$時，測量$f$和$g$之間的重疊。
當為離散物件時，積分就變成求和。例如，對於由索引為$\mathbb{Z}$的、平方可和的、無限維向量集合中抽取的向量，我們得到以下定義：

$$(f * g)(i) = \sum_a f(a) g(i-a).$$

對於二維張量，則為$f$的索引$(a, b)$和$g$的索引$(i-a, j-b)$上的對應加和：

$$(f * g)(i, j) = \sum_a\sum_b f(a, b) g(i-a, j-b).$$
:eqlabel:`eq_2d-conv-discrete`

這看起來類似於 :eqref:`eq_conv-layer`，但有一個主要區別：這裡不是使用$(i+a, j+b)$，而是使用差值。然而，這種區別是表面的，因為我們總是可以匹配 :eqref:`eq_conv-layer`和 :eqref:`eq_2d-conv-discrete`之間的符號。我們在 :eqref:`eq_conv-layer`中的原始定義更正確地描述了*互相關*（cross-correlation），這個問題將在下一節中討論。

## “沃爾多在哪裡”回顧

回到上面的“沃爾多在哪裡”遊戲，讓我們看看它到底是什麼樣子。卷積層根據濾波器$\mathbf{V}$選取給定大小的視窗，並加權處理圖片，如 :numref:`fig_waldo_mask`中所示。我們的目標是學習一個模型，以便探測出在“沃爾多”最可能出現的地方。

![發現沃爾多。](../img/waldo-mask.jpg)
:width:`400px`
:label:`fig_waldo_mask`

### 通道
:label:`subsec_why-conv-channels`

然而這種方法有一個問題：我們忽略了圖像一般包含三個通道/三種原色（紅色、綠色和藍色）。
實際上，圖像不是二維張量，而是一個由高度、寬度和顏色組成的三維張量，比如包含$1024 \times 1024 \times 3$個畫素。
前兩個軸與畫素的空間位置有關，而第三個軸可以看作每個畫素的多維表示。
因此，我們將$\mathsf{X}$索引為$[\mathsf{X}]_{i, j, k}$。由此卷積相應地調整為$[\mathsf{V}]_{a,b,c}$，而不是$[\mathbf{V}]_{a,b}$。

此外，由於輸入圖像是三維的，我們的隱藏表示$\mathsf{H}$也最好採用三維張量。
換句話說，對於每一個空間位置，我們想要採用一組而不是一個隱藏表示。這樣一組隱藏表示可以想象成一些互相堆疊的二維網格。
因此，我們可以把隱藏表示想象為一系列具有二維張量的*通道*（channel）。
這些通道有時也被稱為*特徵對映*（feature maps），因為每個通道都向後續層提供一組空間化的學習特徵。
直觀上可以想象在靠近輸入的底層，一些通道專門識別邊緣，而一些通道專門識別紋理。

為了支援輸入$\mathsf{X}$和隱藏表示$\mathsf{H}$中的多個通道，我們可以在$\mathsf{V}$中新增第四個座標，即$[\mathsf{V}]_{a, b, c, d}$。綜上所述，

$$[\mathsf{H}]_{i,j,d} = \sum_{a = -\Delta}^{\Delta} \sum_{b = -\Delta}^{\Delta} \sum_c [\mathsf{V}]_{a, b, c, d} [\mathsf{X}]_{i+a, j+b, c},$$
:eqlabel:`eq_conv-layer-channels`

其中隱藏表示$\mathsf{H}$中的索引$d$表示輸出通道，而隨後的輸出將繼續以三維張量$\mathsf{H}$作為輸入進入下一個卷積層。
所以， :eqref:`eq_conv-layer-channels`可以定義具有多個通道的卷積層，而其中$\mathsf{V}$是該卷積層的權重。

然而，仍有許多問題亟待解決。
例如，圖像中是否到處都有存在沃爾多的可能？如何有效地計算輸出層？如何選擇適當的啟用函式？為了訓練有效的網路，如何做出合理的網路設計選擇？我們將在本章的其它部分討論這些問題。

## 小結

- 圖像的平移不變性使我們以相同的方式處理區域性圖像，而不在乎它的位置。
- 區域性性意味著計算相應的隱藏表示只需一小部分區域性圖像畫素。
- 在圖像處理中，卷積層通常比全連線層需要更少的引數，但依舊獲得高效用的模型。
- 卷積神經網路（CNN）是一類特殊的神經網路，它可以包含多個卷積層。
- 多個輸入和輸出通道使模型在每個空間位置可以獲取圖像的多方面特徵。

## 練習

1. 假設卷積層 :eqref:`eq_conv-layer`覆蓋的區域性區域$\Delta = 0$。在這種情況下，證明卷積核心為每組通道獨立地實現一個全連線層。
1. 為什麼平移不變性可能也不是好主意呢？
1. 當從圖像邊界畫素獲取隱藏表示時，我們需要思考哪些問題？
1. 描述一個類別似的音訊卷積層的架構。
1. 卷積層也適合於文字資料嗎？為什麼？
1. 證明在 :eqref:`eq_2d-conv-discrete`中，$f * g = g * f$。

[Discussions](https://discuss.d2l.ai/t/5767)
