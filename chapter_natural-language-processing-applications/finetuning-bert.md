# 針對序列級和詞元級應用微調BERT
:label:`sec_finetuning-bert`

在本章的前幾節中，我們為自然語言處理應用設計了不同的模型，例如基於迴圈神經網路、卷積神經網路、注意力和多層感知機。這些模型在有空間或時間限制的情況下是有幫助的，但是，為每個自然語言處理任務精心設計一個特定的模型實際上是不可行的。在 :numref:`sec_bert`中，我們介紹了一個名為BERT的預訓練模型，該模型可以對廣泛的自然語言處理任務進行最少的架構更改。一方面，在提出時，BERT改進了各種自然語言處理任務的技術水平。另一方面，正如在 :numref:`sec_bert-pretraining`中指出的那樣，原始BERT模型的兩個版本分別帶有1.1億和3.4億個引數。因此，當有足夠的計算資源時，我們可以考慮為下游自然語言處理應用微調BERT。

下面，我們將自然語言處理應用的子集概括為序列級和詞元級。在序列層次上，介紹了在單文字分類任務和文字對分類（或迴歸）任務中，如何將文字輸入的BERT表示轉換為輸出標籤。在詞元級別，我們將簡要介紹新的應用，如文字標註和問答，並說明BERT如何表示它們的輸入並轉換為輸出標籤。在微調期間，不同應用之間的BERT所需的“最小架構更改”是額外的全連線層。在下游應用的監督學習期間，額外層的引數是從零開始學習的，而預訓練BERT模型中的所有引數都是微調的。

## 單文字分類

*單文字分類*將單個文字序列作為輸入，並輸出其分類結果。
除了我們在這一章中探討的情感分析之外，語言可接受性語料庫（Corpus of Linguistic Acceptability，COLA）也是一個單文字分類別的資料集，它的要求判斷給定的句子在語法上是否可以接受。 :cite:`Warstadt.Singh.Bowman.2019`。例如，“I should study.”是可以接受的，但是“I should studying.”不是可以接受的。

![微調BERT用於單文字分類應用，如情感分析和測試語言可接受性（這裡假設輸入的單個文字有六個詞元）](../img/bert-one-seq.svg)
:label:`fig_bert-one-seq`

 :numref:`sec_bert`描述了BERT的輸入表示。BERT輸入序列明確地表示單個文字和文字對，其中特殊分類標記“&lt;cls&gt;”用於序列分類，而特殊分類標記“&lt;sep&gt;”標記單個文字的結束或分隔成對文字。如 :numref:`fig_bert-one-seq`所示，在單文字分類應用中，特殊分類標記“&lt;cls&gt;”的BERT表示對整個輸入文字序列的資訊進行編碼。作為輸入單個文字的表示，它將被送入到由全連線（稠密）層組成的小多層感知機中，以輸出所有離散標籤值的分佈。

## 文字對分類或迴歸

在本章中，我們還研究了自然語言推斷。它屬於*文字對分類*，這是一種對文字進行分類別的應用型別。

以一對文字作為輸入但輸出連續值，*語義文字相似度*是一個流行的“文字對迴歸”任務。
這項任務評估句子的語義相似度。例如，在語義文字相似度基準資料集（Semantic Textual Similarity Benchmark）中，句子對的相似度得分是從0（無語義重疊）到5（語義等價）的分數區間 :cite:`Cer.Diab.Agirre.ea.2017`。我們的目標是預測這些分數。來自語義文字相似性基準資料集的樣本包括（句子1，句子2，相似性得分）：

* "A plane is taking off."（“一架飛機正在起飛。”），"An air plane is taking off."（“一架飛機正在起飛。”），5.000分;
* "A woman is eating something."（“一個女人在吃東西。”），"A woman is eating meat."（“一個女人在吃肉。”），3.000分;
* "A woman is dancing."（一個女人在跳舞。），"A man is talking."（“一個人在說話。”），0.000分。

![文字對分類或迴歸應用的BERT微調，如自然語言推斷和語義文字相似性（假設輸入文字對分別有兩個詞元和三個詞元）](../img/bert-two-seqs.svg)
:label:`fig_bert-two-seqs`

與 :numref:`fig_bert-one-seq`中的單文字分類相比， :numref:`fig_bert-two-seqs`中的文字對分類別的BERT微調在輸入表示上有所不同。對於文字對迴歸任務（如語義文字相似性），可以應用細微的更改，例如輸出連續的標籤值和使用均方損失：它們在迴歸中很常見。

## 文字標註

現在讓我們考慮詞元級任務，比如*文字標註*（text tagging），其中每個詞元都被分配了一個標籤。在文字標註任務中，*詞性標註*為每個單詞分配詞性標記（例如，形容詞和限定詞）。
根據單詞在句子中的作用。如，在Penn樹庫II標註集中，句子“John Smith‘s car is new”應該被標記為“NNP（名詞，專有單數）NNP POS（所有格結尾）NN（名詞，單數或品質）VB（動詞，基本形式）JJ（形容詞）”。

![文字標記應用的BERT微調，如詞性標記。假設輸入的單個文字有六個詞元。](../img/bert-tagging.svg)
:label:`fig_bert-tagging`

 :numref:`fig_bert-tagging`中說明了文字標記應用的BERT微調。與 :numref:`fig_bert-one-seq`相比，唯一的區別在於，在文字標註中，輸入文字的*每個詞元*的BERT表示被送到相同的額外全連線層中，以輸出詞元的標籤，例如詞性標籤。

## 問答

作為另一個詞元級應用，*問答*反映閱讀理解能力。
例如，斯坦福問答資料集（Stanford Question Answering Dataset，SQuAD v1.1）由閱讀段落和問題組成，其中每個問題的答案只是段落中的一段文字（文字片段） :cite:`Rajpurkar.Zhang.Lopyrev.ea.2016`。舉個例子，考慮一段話：“Some experts report that a mask's efficacy is inconclusive.However,mask makers insist that their products,such as N95 respirator masks,can guard against the virus.”（“一些專家報告說面罩的功效是不確定的。然而，口罩製造商堅持他們的產品，如N95口罩，可以預防病毒。”）還有一個問題“Who say that N95 respirator masks can guard against the virus?”（“誰說N95口罩可以預防病毒？”）。答案應該是文章中的文字片段“mask makers”（“口罩製造商”）。因此，SQuAD v1.1的目標是在給定問題和段落的情況下預測段落中文字片段的開始和結束。

![對問答進行BERT微調（假設輸入文字對分別有兩個和三個詞元）](../img/bert-qa.svg)
:label:`fig_bert-qa`

為了微調BERT進行問答，在BERT的輸入中，將問題和段落分別作為第一個和第二個文字序列。為了預測文字片段開始的位置，相同的額外的全連線層將把來自位置$i$的任何詞元的BERT表示轉換成標量分數$s_i$。文章中所有詞元的分數還透過softmax轉換成機率分佈，從而為文章中的每個詞元位置$i$分配作為文字片段開始的機率$p_i$。預測文字片段的結束與上面相同，只是其額外的全連線層中的引數與用於預測開始位置的引數無關。當預測結束時，位置$i$的詞元由相同的全連線層變換成標量分數$e_i$。 :numref:`fig_bert-qa`描述了用於問答的微調BERT。

對於問答，監督學習的訓練目標就像最大化真實值的開始和結束位置的對數似然一樣簡單。當預測片段時，我們可以計算從位置$i$到位置$j$的有效片段的分數$s_i + e_j$（$i \leq j$），並輸出分數最高的跨度。

## 小結

* 對於序列級和詞元級自然語言處理應用，BERT只需要最小的架構改變（額外的全連線層），如單個文字分類（例如，情感分析和測試語言可接受性）、文字對分類或迴歸（例如，自然語言推斷和語義文字相似性）、文字標記（例如，詞性標記）和問答。
* 在下游應用的監督學習期間，額外層的引數是從零開始學習的，而預訓練BERT模型中的所有引數都是微調的。

## 練習

1. 讓我們為新聞文章設計一個搜尋引擎演算法。當系統接收到查詢（例如，“冠狀病毒爆發期間的石油行業”）時，它應該返回與該查詢最相關的新聞文章的排序列表。假設我們有一個巨大的新聞文章池和大量的查詢。為了簡化問題，假設為每個查詢標記了最相關的文章。如何在演算法設計中應用負取樣（見 :numref:`subsec_negative-sampling`）和BERT？
1. 我們如何利用BERT來訓練語言模型？
1. 我們能在機器翻譯中利用BERT嗎？

[Discussions](https://discuss.d2l.ai/t/5729)
